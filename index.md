# AI in Self-Driving Cars

In the past, people thought that today's world would have flying cars. Now people in today's world think that the future will see self-driving cars. Is this idea a futuristic dream or an idea that will soon become a reality? Many cars now have embedded artificial intelligence, but what works well or causes problems? What can the future of self-driving cars hold, and what are the effects of implementing self-driving features through the use of AI in vehicles?

## Current State
Within the last 20 years, vehicle manufacturers have been developing features to make driving safer. However, human error still exists, so autonomous vehicle manufactorers have good reason to develop better AI for their cars. Distracted driving is a huge problem on the roads, but an autonomous vehicle cannot be distracted. Most people would consider a self-driving car to not require a person in the driver's seat, which is a point that has not been reached yet. However, many cars driven by a person could be considered partially autonomous. Newer cars often come with blind-spot monitoring, lane-keep assist, self-park, and other features that are possible because of an artificial intelligence component. These features, though much more subtle than a full autonomy, can help to prevent accidents. These features provide the driver with more knowledge of their surroundings, but the driver is still in control of the vehicle. There are also features like alerts for drivers who drift out of their lane multiple times in a given time period. The car is able to alert them to stay more awake. These features are a long ways away from a self-driving car, but they are a big step in the right direction. Tesla vehicles are much more advanced than cars with these features. Tesla cars are able to drive somewhat autonomously, but they still require a driver because they are far from perfect. A tesla can detect obstacles and know when to accelerate or stop. Teslas use neural networks on problems ranging from perception to control. Their cameras analyze raw images to perform semantic segmentation, object detection, and monocular depth estimation. Despite all the training, Teslas still require a human behind the wheel. There are always unexpected cases that have not been trained. A common question is what an autonomous car would do if a pedestrain jumped in front of the car when there was a wall on one side of the vehicle and another pedestrian on the other side. The developers of the artificial intelligence driving the car would have to choose between the lives of each pedestrian and the driver. It is not clear if any person would die from a given course of action, but the vehicle must make a choice. This is why current self-driving vehicles say that the driver should remain alert and in control of the vehicle. The features are there as an aid, not something to be dependent upon. This raises an ethical question of which life the car chooses and whether or not a developer has the right to choose that. It could also raise the question of who is at fault, though currently, the fault would be to the driver as they are told to stay in control of the vehicle. 
 

### Shortcomings
About 9.1 crashes occur per million miles driven in autonomous vehicles. Regular vehicles have about 4.1 crashes per million miles driven. This would demonstrate that self-driving cars are more dangerous than regular vehicles. However, there are other factors to consider. Self-driving cars typically cause fewer severe injuries. Just as tired drivers cause worse accidents than drunk drivers (since drunk drivers respond late, but drivers asleep at the wheel do not respond at all), a self-driving car may not be able to prevent an accident, but it could lessen the severity. A self-driving car does not become distracted or fall asleep at the wheel, but it can fail to perceive something in its surroundings, or it could come upon a scenario for which it has not been trained. Despite 48 networks with 70,000 GPU training hours, Tesla autopilot vehicles cannot currently be trained for any scenario. The question arises - can autonomous vehicles be trained fully enough to fully replace regular vehicles?

## Future Possibilities


## Ethical Implications
Self-driving cars bring about a plethora of ethical questions. Who should decide which life to save in different scenarios? Who is to blame for an accident involving a self-driving car? While self-driving cars seem to lessen the severity of accidents, they can still crash. At what point is it ethically better to switch to a self-driving car? Is it simply when self-driving cars result in less accidents, or do they need to have a perfect track record with zero accidents? Who gets to make these decisions? The state of autonomous vehicles currently gives us time to think about these questions. Fully driverless cars are not being thrown onto the roads with no regulation, but the technology is still developing. We are likely to come to a point where these questions will need to be answered, and developers will have to ensure their vehicles meet a certain standard. 


### Benefits

### Detriments

## Why Should You Care?
include a call to action or ground the importance in other topics the reader would care about

## Sources
TODO: change citation method and order
* https://www.techtarget.com/searchenterpriseai/definition/driverless-car#:~:text=AI%20software%20in%20the%20car,such%20as%20steering%20and%20brakes.
* https://www.micron.com/insight/on-the-road-to-full-autonomy-self-driving-cars-will-rely-on-ai-and-innovative-memory
* https://www.tesla.com/AI
* https://carsurance.net/insights/self-driving-car-statistics/
